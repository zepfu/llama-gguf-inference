# llama-gguf-inference Environment Configuration
# Copy this to .env and customize for your deployment

# ==============================================================================
# AUTHENTICATION
# ==============================================================================

# Enable/disable API key authentication
AUTH_ENABLED=true

# Path to API keys file (key_id:api_key format)
AUTH_KEYS_FILE=/data/api_keys.txt

# Rate limit per key_id (requests per minute)
MAX_REQUESTS_PER_MINUTE=100

# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================

# Option 1: Model filename (looks in MODELS_DIR)
MODEL_NAME=your-model.gguf

# Option 2: Full path to model file (alternative to MODEL_NAME)
# MODEL_PATH=/custom/path/to/model.gguf

# ==============================================================================
# DIRECTORIES
# ==============================================================================

# Base directory for models and logs
# Auto-detects: /runpod-volume, /workspace, or /data
DATA_DIR=/data

# Models directory (if using MODEL_NAME)
MODELS_DIR=/data/models

# ==============================================================================
# PORTS
# ==============================================================================

# Public gateway port (API endpoints)
PORT=8000

# Health check port (for platform monitoring)
PORT_HEALTH=8001

# Internal llama-server port
PORT_BACKEND=8080

# Host to bind (usually 0.0.0.0 for containers)
HOST=0.0.0.0

# ==============================================================================
# INFERENCE CONFIGURATION
# ==============================================================================

# GPU layers to offload (99 = all layers, 0 = CPU only)
NGL=99

# Context length (max tokens)
CTX=16384

# CPU threads (0 = auto-detect)
THREADS=0

# Additional llama-server arguments (optional)
# EXTRA_ARGS="--verbose --temp 0.7"

# ==============================================================================
# LOGGING
# ==============================================================================

# Worker type for log organization (optional)
# Examples: instruct, coder, omni, chat, completion
# Logs will go to: /data/logs/llama-{WORKER_TYPE}/
# Leave empty for default: /data/logs/llama/
WORKER_TYPE=

# Log directory name (overrides default)
# LOG_NAME=llama

# ==============================================================================
# DEBUG
# ==============================================================================

# Hold container without starting services (for inspection)
DEBUG_SHELL=false

# ==============================================================================
# PLATFORM-SPECIFIC EXAMPLES
# ==============================================================================

# Local Development
# DATA_DIR=/data
# MODEL_NAME=test-model.gguf
# AUTH_ENABLED=false
# NGL=0  # CPU only for testing

# RunPod Serverless
# DATA_DIR=/runpod-volume  # Auto-detected
# MODEL_NAME=production-model.gguf
# AUTH_ENABLED=true
# PORT_HEALTH=8001  # For scale-to-zero

# Production
# MODEL_NAME=production-model-Q4_K_M.gguf
# AUTH_ENABLED=true
# AUTH_KEYS_FILE=/secrets/api_keys.txt
# WORKER_TYPE=production
# NGL=99
# CTX=16384
# MAX_REQUESTS_PER_MINUTE=200

# ==============================================================================
# NOTES
# ==============================================================================
# 
# - Copy api_keys.txt.example to $AUTH_KEYS_FILE and add your keys
# - Set AUTH_ENABLED=false for initial testing
# - Health endpoints (/ping, /health, /metrics) never require auth
# - All /v1/* endpoints require valid API key when auth is enabled
# - Use WORKER_TYPE to organize logs by deployment type
# - Logs use timestamp-first format for easy chronological sorting
#
