# ==============================================================================
# llama-gguf-inference â€” CPU (multi-arch: amd64, arm64)
#
# GGUF model inference server using llama.cpp with CPU-only inference.
# Supports linux/amd64 and linux/arm64 for broad platform compatibility.
#
# Base image: ghcr.io/ggml-org/llama.cpp:server (multi-arch CPU)
# For NVIDIA GPU acceleration, see Dockerfile (CUDA, amd64 only)
# ==============================================================================

FROM ghcr.io/ggml-org/llama.cpp:server

ARG GIT_SHA=unknown
ARG BUILD_TIME=unknown
ARG TARGETPLATFORM

LABEL org.opencontainers.image.title="llama-gguf-inference" \
      org.opencontainers.image.description="GGUF model inference server using llama.cpp (CPU)" \
      org.opencontainers.image.revision="$GIT_SHA" \
      org.opencontainers.image.created="$BUILD_TIME" \
      org.opencontainers.image.source="https://github.com/zepfu/llama-gguf-inference" \
      org.opencontainers.image.variant="cpu"

# Set architecture-aware library path
# TARGETPLATFORM is provided by Docker buildx (e.g., linux/amd64, linux/arm64)
RUN if [ "$TARGETPLATFORM" = "linux/arm64" ]; then \
        echo "Building for ARM64"; \
        LIB_ARCH="aarch64-linux-gnu"; \
    else \
        echo "Building for AMD64"; \
        LIB_ARCH="x86_64-linux-gnu"; \
    fi && \
    echo "LD_LIBRARY_PATH=/app:/usr/local/lib:/usr/lib/${LIB_ARCH}" > /etc/environment

ENV LD_LIBRARY_PATH=/app:/usr/local/lib

# Minimal Python for gateway (uses stdlib only, no pip packages)
# hadolint ignore=DL3008
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends \
        python3 \
        ca-certificates \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* /var/tmp/*

# Set working directory
WORKDIR /opt/app

# Copy only runtime scripts (not tests, diagnostics, benchmarks, docs)
# Selective COPY avoids relying solely on .dockerignore and makes the image
# contents explicit. Only these files are needed at runtime:
#   start.sh         - Container entrypoint (orchestrates all services)
#   gateway.py       - API gateway (port 8000)
#   auth.py          - API key authentication and rate limiting
#   health_server.py - Platform health checks (port 8001)
#   key_mgmt.py      - API key management CLI
COPY scripts/start.sh scripts/gateway.py scripts/auth.py scripts/health_server.py scripts/key_mgmt.py /opt/app/scripts/

# Write version file and set permissions in a single layer
# hadolint ignore=SC2015
RUN printf "GIT_SHA=%s\nBUILD_TIME=%s\nPLATFORM=%s\n" "$GIT_SHA" "$BUILD_TIME" "$TARGETPLATFORM" > /opt/app/VERSION \
    && find /opt/app/scripts -name '*.sh' -exec chmod +x {} + 2>/dev/null; \
       find /opt/app/scripts -name '*.py' -exec chmod +x {} + 2>/dev/null; \
       if ! test -x /app/llama-server; then echo "ERROR: /app/llama-server not found"; exit 1; fi

# Default environment
# NGL=0 for CPU-only inference (no GPU layer offloading)
# DATA_DIR: Base path for models and logs
#   - Auto-detects /runpod-volume or /workspace if /data doesn't exist
#   - Override for custom setups: DATA_DIR=/mnt/storage
ENV DATA_DIR=/data \
    PORT=8000 \
    BACKEND_PORT=8080 \
    NGL=0 \
    CTX=4096 \
    LOG_NAME=llama

EXPOSE 8000

# Health check using Python stdlib (no curl dependency)
HEALTHCHECK --interval=30s --timeout=3s --start-period=60s --retries=3 \
    CMD python3 -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/ping')" || exit 1

ENTRYPOINT ["/opt/app/scripts/start.sh"]
